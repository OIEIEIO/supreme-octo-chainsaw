{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Zoetrope 5.5 -OIEIEIO",
      "provenance": [],
      "collapsed_sections": [
        "1C_Yq-rvgxbZ",
        "TjeUXwyLTjQx",
        "KAcixx9Z3XYH",
        "GHB7BcR1zNLZ",
        "XaocGDQXz3Zx",
        "WztSrRF23Rqg"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OIEIEIO/supreme-octo-chainsaw/blob/main/Copy_of_Zoetrope_5_5_OIEIEIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2qJluQrTky4"
      },
      "source": [
        "# License\n",
        "Copyright (c) 2021 Bearsharktopusdev\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQULmg_lpMNq"
      },
      "source": [
        "# How do I use this?\n",
        "\n",
        "First, type in your description(s) where it says *A beautiful brutalist Waluigi* or upload image(s) and enter their path. You can add multiple text and image paths, and/or you can leave them blank. \n",
        "\n",
        "Next, decide how much weight you want each text and image to have on the output by adjusting the `w` variables -- it is suggested that you set at least one weight to 1.\n",
        "\n",
        "Finally, click **Runtime** up in the top menu and choose **Restart and Run All**, then wait as each sequential play button circles around.\n",
        "\n",
        "Your output will appear at the bottom of this page near the **Output** heading as it processes after a short while. Scroll down below everything else to see new images appear. \n",
        "\n",
        "The images will start by looking like dirt, but the page will eventually ding and show new images as it begins to attempt to match the image to your inputs.\n",
        "\n",
        "---\n",
        "\n",
        "## What's new between Zoetrope 5 and Zoetrope 5.5?\n",
        "The project has taken on a new assistant - Dekxi! Their help has been instrumental in getting post-processing up and running, and will be similarly helpful for the upcoming camera zoom/pan/rotate (general affine transformation) features. A couple more minor features, tweaks, and general codebase changes have been made. This thing is sort of beginning to spiral out of control, so I'm going to spend some time after 6.0 trying to clean it up so we don't accumulate too much code debt.\n",
        "<br><br>\n",
        "\n",
        "__New major feature__: Postprocessing options<br>\n",
        "__New major feature__: CLIP and VQGAN models both separately selectable (under Generation Options)<br>\n",
        "_New minor feature_: Improved model downloading pipeline<br>\n",
        "_New minor feature_: Changed every mention of `/content/` to `default_path` - should make life easier for people looking to run locally.\n",
        "<br><br>\n",
        "\n",
        "If you'd like to help fund further development of Zoetrope (I would **love** to make this my day job), you can contribute to my [Patreon](https://www.patreon.com/bearsharktopus) (alt link: https://www.patreon.com/bearsharktopus) for access to newer versions faster at $5 and above, or purchase some of my art on my [Redbubble store](http://bearsharktopus.redbubble.com/) (alt link: http://bearsharktopus.redbubble.com/). Any financial assistance is greatly appreciated.<br><br>\n",
        "\n",
        "Also, feel free to join the [Official Zoetrope Discord](https://discord.gg/QPxEB8fcrh), located at https://discord.gg/QPxEB8fcrh!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw0KBLebMywW"
      },
      "source": [
        "# Params\n",
        "\n",
        "Fill in the texts and image paths.\n",
        "\n",
        "**To upload images, just click this button: ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACIAAAAhCAYAAAC803lsAAAAt0lEQVRYCe2WSwrFIAxFu1VX4kYcO3YjuhvHeTjIJDQfrA9auIKEKE2PR7G96CXtegkHAUTuBIzAiDQg82+dkd47tdZu+5xTLm4rd43UWimlpPacM52AcUEsCJ5bMJoxHh9jmKaOgDCQF0spKkwYhFe2GxlSIwmDaAWi4wCRpmAERqQBmeOM/M3I7o3Kzz3eGu/ryy+IxFVLa+4Vvx60/kd4xV5cNawWArEKnJoDiDQJIzAiDcj8By5FgI6MnzeuAAAAAElFTkSuQmCC) then drag them into the left pane.\n",
        "Images default to being uploaded to `/content/<image_name.png>`. ***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@markdown Input your text prompt here and assign it a weight. text_input and text_to_add are two separate prompts with separate weights (w0 and w1).\n",
        "text_input = \"a faster than light spacecraft\" #@param {type:\"string\"}\n",
        "w0 = 0.4 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "text_to_add = \"\" #@param {type:\"string\"}\n",
        "w1 = 0.3 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "\n",
        "# init_image = False #@param {type:\"boolean\"}\n",
        "#@markdown Enter a comma-space (\", \") separated list of image locations as \"image.jpg, image2.jpg, image3.jpg\" to use image-based prompting\n",
        "img_enc_path = \"\" #@param {type:\"string\"}\n",
        "w2 = 0.2 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "img_mode = 'Average'\n",
        "random_list_encode = 0\n",
        "ne_img_enc_path = \"\" #@param {type:\"string\"}\n",
        "w3 = 0.1 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "ne_img_mode = 'Average'\n",
        "ne_random_list_encode =  0\n",
        "#@markdown Enter up to two gifs/mp4s if desired, in each path\n",
        "gif_img_enc_path = \"\" #@param {type:\"string\"}\n",
        "w4 = 0 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "gif2_img_enc_path = \"\" #@param {type:\"string\"}\n",
        "w5 = 0 #@param {type:\"slider\", min:-5, max:5, step:0.1}\n",
        "\n",
        "#@markdown #Startup Options\n",
        "#@markdown Leave init_image_path blank to generate from noise.\n",
        "default_path = '/content/'\n",
        "init_image_path = '' #@param {type:\"string\"}\n",
        "init_image_path = default_path + init_image_path\n",
        "\n",
        "init_type = \"perlin\" #@param [\"image\", \"blocky\", \"perlin\", \"constant\", \"default\"] {type:\"string\"}\n",
        "if len(init_image_path) > len(default_path):\n",
        "  init_type = \"image\"\n",
        "\n",
        "\n",
        "#@markdown #Image Options\n",
        "size_preset = 'Medium Square' #@param [\"N/A\", \"Medium Square\", \"Large Square\", \"Small Square\", \"Large 16:9\", \"Small 16:9\", \"Large 8:9\", \"Small 8:9\", \"Large 4:3\", \"Small 4:3\", \"MTG Card Art\", \"Perceptor Size\"] {type:\"string\"}\n",
        "sideX =  384#@param {type:\"integer\", min: 224, max:2048, step:1}\n",
        "sideY =  682#@param {type:\"integer\", min: 224, max:2048, step:1}\n",
        "\n",
        "channels = 3\n",
        "\n",
        "if size_preset == 'Medium Square':\n",
        "  sideX = 512\n",
        "  sideY = 512\n",
        "elif size_preset == 'Large Square':\n",
        "  sideX = 768\n",
        "  sideY = 768\n",
        "elif size_preset == 'Small Square':\n",
        "  sideX = 256\n",
        "  sideY = 256\n",
        "elif size_preset == 'Large 16:9':\n",
        "  sideX = 576\n",
        "  sideY = 1024\n",
        "elif size_preset == 'Small 16:9':\n",
        "  sideX = 288\n",
        "  sideY = 512\n",
        "elif size_preset == 'Large 8:9':\n",
        "  sideX = 720\n",
        "  sideY = 640\n",
        "elif size_preset == 'Small 8:9':\n",
        "  sideX = 360\n",
        "  sideY = 320\n",
        "elif size_preset == 'Large 4:3':\n",
        "  sideX = 672\n",
        "  sideY = 896\n",
        "elif size_preset == 'Small 4:3':\n",
        "  sideX = 336\n",
        "  sideY = 448\n",
        "elif size_preset == 'MTG Card Art':\n",
        "  sideX = 624\n",
        "  sideY = 848\n",
        "elif size_preset == 'Perceptor Size':\n",
        "  sideX = perceptor_size\n",
        "  sideY = perceptor_size\n",
        "else:\n",
        "  im_shape = [512, 512, 3]\n",
        "\n",
        "im_shape = [sideX, sideY, channels]\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "#@markdown #Generation Options:\n",
        "#@markdown Defaults are ViT-B/32 and imagenet_16384.\n",
        "clip_model = \"ViT-B/32\" #@param ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32','ViT-B/16']\n",
        "vqgan_model = \"imagenet_16384\" #@param ['imagenet_16384', 'imagenet_1024', 'openimages_8192', 'coco', 'faceshq', 'wikiart_1024', 'wikiart_16384', 'sflckr', 'gumbel']\n",
        "\n",
        "learning_epochs = 1#@param {type: \"integer\"}\n",
        "save_rate =  100#@param {type:\"integer\"}\n",
        "display_rate =  100#@param {type:\"integer\"}\n",
        "single_display = False #@param {type:\"boolean\"}\n",
        "auto_video = False #@param {type:\"boolean\"}\n",
        "out_folder = 'l25v_output' #@param{type:\"string\"}\n",
        "# If you need to create the folder:\n",
        "\n",
        "#@markdown #Augmentation Options:\n",
        "#@markdown Augmentations Per Iteration. Turn this down if you're running out of memory.\n",
        "cutN =   32#@param {type:\"integer\", min: 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C_Yq-rvgxbZ"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igOePBpAg2FU"
      },
      "source": [
        "#The following is a list of postprocessing options available to the end user.\n",
        "\n",
        "#Postprocessing List\n",
        "# The below list applies each postprocessing feature in the order that they are \n",
        "# listed to any image output.\n",
        "# Example: postprocessing_list = [\"saturation\", \"contrast\", \"brightness\"]\n",
        "# Saturation is run first, followed by contrast, followed by brightness\n",
        "postprocessing_list = [\"\"]\n",
        "\n",
        "#Palettize (\"palettize\")\n",
        "# Squishes all the color down in the image into the applied palette or palette preset.\n",
        "# Palette data must be in a specific format - a list of integers, in groups of three (RGB values), that are an exponent of 2\n",
        "# 2, 4, 8, 16, etc. You can use duplicates to achieve non-exponent-of-2 numbers of colors [0,0,0, 0,0,0, 128,128,128, 255,255,255]\n",
        "palette = [0,0,0, 255,0,0, 0,255,0, 0,0,255, 0,255,255, 255,0,255, 255,255,0, 255,255,255] #RGB + CMY + Black/White\n",
        "palettize_dithering = True\n",
        "#TODO\n",
        "#palette_preset = None # [\"gameboy classic\", \"gameboy advance\", \"nes\", \"RGB+Black/White\", \"CYMK\", \"RGB+CMY+Black/White\"]\n",
        "\n",
        "#Quantize (\"quantize\")\n",
        "# Like a more automated Palettize. Squishes down an image's colors into only a certain amount.\n",
        "quantize_number = 64\n",
        "quantize_dithering = True\n",
        "\n",
        "#Pixelate (\"pixelate\")\n",
        "# Shrinks an image and then upscales it back up in order to pixelate it.\n",
        "# Works best with square images and with exponents of two. May break on weird resolutions\n",
        "pixelate_factor = 4 #[2, 4, 8, 16, 32]\n",
        "\n",
        "#Brightness (\"brightness\")\n",
        "# Alters an image's darkness/brightness level. \n",
        "# Numbers between 0 and 1 darken, numbers above 1 brighten.\n",
        "brightness_factor = 1.0\n",
        "\n",
        "#Contrast (\"contrast\")\n",
        "# Alters an image's contrast level. \n",
        "# Numbers between 0 and 1 decrease contrast, numbers above 1 increase contrast.\n",
        "contrast_factor = 1.0\n",
        "\n",
        "#Saturation (\"saturation\")\n",
        "# Alters an image's saturation level (colorfulness)\n",
        "# Numbers between 0 and 1 decrease saturation, numbers above 1 increase saturation.\n",
        "saturation_factor = 1.0\n",
        "\n",
        "#Sharpness (\"sharpness\")\n",
        "# Alters an image's sharpness level. \n",
        "# Numbers between 0 and 1 decrease sharpness, numbers above 1 increase sharpness.\n",
        "sharpness_factor = 1.0\n",
        "\n",
        "#Box Blur (\"box_blur\")\n",
        "# Blurs an image using a square box of radius n pixels to generate an average value\n",
        "box_blur_radius = 0.0\n",
        "\n",
        "#Invert (\"invert\")\n",
        "# Inverts an images colors.\n",
        "# No parameters.\n",
        "\n",
        "#Grayscale (\"grayscale\")\n",
        "# Turns an image greyscale.\n",
        "# No parameters.\n",
        "\n",
        "#Mirror (\"mirror\")\n",
        "# Mirrors an image horizontally. Not sure why you want to use this, but okay.\n",
        "# No parameters.\n",
        "\n",
        "#Flip (\"flip\")\n",
        "# Flips an image vertically.\n",
        "# No parameters.\n",
        "\n",
        "\n",
        "\n",
        "#TODOS\n",
        "#These are nonfunctional.\n",
        "\n",
        "#Gamma\n",
        "# Applies gamma correction to the image\n",
        "# TODO: This is not functional, requires opencv->PIL passing back and forth.\n",
        "\n",
        "#Sepia\n",
        "# Applies a sepia filter to the image\n",
        "# TODO: Requires writing some definition stuff. See: https://www.yabirgb.com/sepia_filter/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjeUXwyLTjQx"
      },
      "source": [
        "# Advanced Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BaAF1pufXa1Y"
      },
      "source": [
        "#@markdown **Initialization Options**\n",
        "extra_fuzz_strength = 0.6 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "grayscale_fuzz = True #@param {type:\"boolean\"}\n",
        "init_val = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "# These alter the starting noise generated for your prompt.\n",
        "# They are overridden if init_image is checked.\n",
        "# blocky_random = False # default False\n",
        "grayscale_random = False #@param {type:\"boolean\"}\n",
        "random_size = 2048 #@param {type:\"slider\", min:2, max:2048, step:1}\n",
        "perlin_scale = 1 #@param {type:\"slider\", min:0.1, max:100, step:0.1}\n",
        "perlin_strength = 0.5 #@param {type:\"slider\", min:0, max:2, step:0.01}\n",
        "#How many octaves to use?\n",
        "perlin_octaves =  8#@param {type:\"integer\", min: 1}\n",
        "# How much to decay each octave? (higher = more high freq noise)\n",
        "perlin_persistence = 0.65 #@param {type:\"slider\", min:0.1, max:2, step:0.1}\n",
        "#How mucch to scale each octave\n",
        "perlin_lacunarity = 2  #@param {type:\"slider\", min:0.1, max:5, step:0.1}\n",
        "\n",
        "#@markdown **Learning Schedule**\n",
        "learning_rate =  0.30#@param {type:\"number\"}\n",
        "min_learning_rate =  0.15#@param {type:\"number\"}\n",
        "learning_method = \"BOIL\" #@param [\"DEFAULT\", \"BOIL\", \"SEAR\", \"RANDOM\"] {type:\"string\"}\n",
        "decay =  0.1#@param {type:\"number\"}\n",
        "epsilon =  0#@param {type:\"number\"}\n",
        "optimizer_type = \"AdamW\" #@param [\"AdamW\", \"Ranger21\", \"AccSGD\",\"AdaBound\",\"AdaMod\",\"Adafactor\",\"AdamP\",\"AggMo\",\"DiffGrad\",\"Lamb\",\"NovoGrad\",\"PID\",\"QHAdam\",\"QHM\",\"RAdam\",\"SGDP\",\"SGDW\",\"Shampoo\",\"SWATS\",\"Yogi\"] {type:\"string\"}\n",
        "#@markdown Use Lookahead will enable an additional optimizer that attempts to \"look ahead\" for further image optimization.\n",
        "use_lookahead = True #@param {type:\"boolean\"}\n",
        "#@markdown **Output Options**<br>\n",
        "#@markdown learning_epochs is the number of times the training loop runs for total_iterations steps. After that many steps, it will reset itself and begin generating from scratch again.\n",
        "total_iterations =  400#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Random rotation degrees\n",
        "deg = 12 #@param {type:\"slider\", min:0, max:90, step:1}\n",
        "#@markdown Mirroring Probabilities\n",
        "horizontal = 0.1 #@param {type:\"slider\", min:0.0, max:0.5,step:0.01}\n",
        "vertical = 0.02 #@param {type:\"slider\", min:0.0, max:0.5,step:0.01}\n",
        "#@markdown Augmentation Noise\n",
        "random_noise = 0 #@param {type:\"slider\", min:0.0, max:5, step:0.01}\n",
        "random_erasing = 0 #@param {type:\"slider\", min:0.0, max:1, step:0.01}\n",
        "blur_probability = 0.1 #@param {type:\"slider\", min:0.0, max:1, step:0.01}\n",
        "#@markdown Sharpness filter settings (experimental)\n",
        "sharpen_pre_augment = False #@param {type:\"boolean\"}\n",
        "sharpen_post_augment = False #@param {type:\"boolean\"}\n",
        "sharpen_every =   1#@param {type:\"integer\", min: 0}\n",
        "#@markdown Latent vector augmentation settings (experimental)\n",
        "lats_nonlinearity = \"tanh\" #@param [\"clip\", \"tanh\", \"none\"] {type:\"string\"}\n",
        "lats_noise =  0 #@param {type:\"slider\", min:0.0, max:0.1, step:0.001}\n",
        "lats_scale = 10 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
        "\n",
        "optim_cfg = {\n",
        "              \"optimizer_type\" : optimizer_type,\n",
        "              \"learning_rate\" : learning_rate,\n",
        "              \"min_learning_rate\" : min_learning_rate,\n",
        "              \"learning_method\": learning_method,\n",
        "              \"decay\" : decay,\n",
        "              \"iterations\" : total_iterations,\n",
        "              \"model_type\" : \"VQGAN_16384\", #for now this does nothing. TODO: Switch models somehow but keep the same image?\n",
        "              \"use_lookahead\" : use_lookahead,\n",
        "              \"epsilon\" : epsilon\n",
        "}\n",
        "optim_chain = [[optim_cfg]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYZ36ebsnaA7"
      },
      "source": [
        "#Advanced Feature: Music Enabling\n",
        "# If you want to use Music Prompts, you have to enable them here\n",
        "# It breaks if the git clone and python setup are in the same cell\n",
        "# I don't know why\n",
        "music_enabled = False\n",
        "if music_enabled:\n",
        "  !git clone https://github.com/jordipons/musicnn.git\n",
        "  %cd musicnn\n",
        "  !python setup.py install\n",
        "  %cd ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwua0Y49TZf8"
      },
      "source": [
        "#Advanced Text Prompts\n",
        "# Every prefix, main prompt, and suffix is multiplied together.\n",
        "# Example:\n",
        "# prefixes = [\"painting\", \"photograph\"]\n",
        "# prompt_main = [\"of a forest\"]\n",
        "# suffixes = [\"by zdzislaw beksinski\", \"\"]\n",
        "# final results are \"painting of a forest by zdzislaw beksinski\",\n",
        "# \"photograph of a forest by zdzislaw beksinski\", \"painting of a forest\",\n",
        "# and \"photograph of a forest\".\n",
        "# Advanced text prompts are treated as if they have a weight of 1 for now.\n",
        "advanced_text_enabled = False #Set to true if you want to use advanced text prompts\n",
        "prefixes       = [\"\"]\n",
        "prompt_main    = [\"\"]\n",
        "infixes        = [\"\"]\n",
        "suffixes       = [\"\"]\n",
        "prompt = []\n",
        "for i in range(len(prefixes)):\n",
        "    for j in range(len(prompt_main)):\n",
        "        for k in range(len(infixes)):\n",
        "            for l in range(len(suffixes)):\n",
        "              prompt_merge = prefixes[i] + \" \" + prompt_main[j] + \" \" + infixes[k] + \" \" + suffixes[l]\n",
        "              prompt.append(prompt_merge)\n",
        "\n",
        "#text_not\n",
        "# text_not is a list of stuff that the system attempts to remove from your images\n",
        "# The default list is '''incoherent, confusing, cropped, watermarks, text, writing'''\n",
        "# Alter if desired!\n",
        "text_not = '''incoherent, confusing, cropped, watermarks, text, writing'''\n",
        "\n",
        "#Warmup & Boil Period\n",
        "# Computes the required multiplier so that a warmup/boil learning rate reaches\n",
        "# a rate of multiplying by (mult) every x steps. \n",
        "warmup_mult = 2 # 2 = Double the learning rate...\n",
        "warmup_period = 300 # Every 200 steps.\n",
        "warmup_amt = warmup_mult**(1/warmup_period)\n",
        "boil_mult = 0.5 # 0.5 = Halve the learning rate...\n",
        "boil_period = 100 # Every 100 steps.\n",
        "boil_amt = boil_mult**(1/boil_period)\n",
        "print(warmup_amt)\n",
        "print(boil_amt)\n",
        "\n",
        "#Blue Noise\n",
        "# Blue Noise is a finer grain form of noise applied as an augmentation.\n",
        "blue_noise_intensity = 0.05 # default 0.05\n",
        "blue_noise_percentage = 0.1 # default 0.1\n",
        "\n",
        "#Display Initial Image on New Epoch\n",
        "# Does what it says it will do.\n",
        "display_init_on_epoch = True\n",
        "\n",
        "#\n",
        "##\n",
        "###\n",
        "#Experimental Parameters - possibly buggy! Handle with care!\n",
        "###\n",
        "##\n",
        "#\n",
        "\n",
        "#Music Prompts\n",
        "# Highly experimental feature!\n",
        "# Generates a list of tags based on a provided MP3,\n",
        "# appends them to the prefix, and then feeds them\n",
        "# as a text-based prompt to the generator.\n",
        "music_prompt = []\n",
        "if music_enabled:\n",
        "  music_title = 'mister brightside'\n",
        "  music_artist = 'the killers'\n",
        "  music_name = default_path + music_title + '.mp3' # Music File Goes Here\n",
        "\n",
        "  import musicnn\n",
        "  from musicnn import tagger\n",
        "  m_taglist = musicnn.tagger.top_tags(music_name, model='MSD_musicnn_big', topN=3)\n",
        "  m_taglist2 = musicnn.tagger.top_tags(music_name, model='MTT_musicnn', topN=3)\n",
        "  m_taglist3 = musicnn.tagger.top_tags(music_name, model='MTT_vgg', topN=3)\n",
        "  m_tags = []\n",
        "  for i in m_taglist:\n",
        "    for k in m_taglist3:\n",
        "      temp = i + \" \" + k \n",
        "      m_tags.append(temp)\n",
        "  for i in m_taglist2:\n",
        "    for k in m_taglist3:\n",
        "      temp = i + \" \" + k \n",
        "      m_tags.append(temp)\n",
        "\n",
        "  print(m_tags)\n",
        "\n",
        "  m_prefixes = [\"album art for\"]\n",
        "  m_infixes = [\"song titled \" + music_title, music_title, \"song named \" + music_title, \"music single named \" + music_title]\n",
        "  m_suffixes = ['']\n",
        "  if music_artist != '':\n",
        "    m_suffixes = ['', 'by ' + music_artist, 'performed by ' + music_artist]\n",
        "  for i in range(len(m_prefixes)):\n",
        "      for j in range(len(m_tags)):\n",
        "          for l in range(len(m_infixes)):\n",
        "            for k in range(len(m_suffixes)):\n",
        "              prompt_merge = m_prefixes[i] + \" \" + m_tags[j] + \" \" + m_infixes[l] + \" \" + m_suffixes[k]\n",
        "              music_prompt.append(prompt_merge)\n",
        "\n",
        "#Scaler\n",
        "# It's sort of like zooming out. Unpredictable effects. Play around with it!\n",
        "# Warning: will fuck up human faces.\n",
        "# Cannot be used with sharpening.\n",
        "scaler = 1.0\n",
        "\n",
        "#RHW Style Loss\n",
        "# Uses a cosine similarity addition function inspired by RiversHaveWings's notebooks.\n",
        "# Enhances the effects of images - an image's weight is functionally multiplied by 3-4,\n",
        "# So be careful and adjust yoour weights accordingly.\n",
        "rhw_style_loss = False\n",
        "\n",
        "#Soft Histogram\n",
        "# It makes a histogram of pixel data to arrange the image by. Play around with it!\n",
        "use_softhistogram = False\n",
        "\n",
        "#JPEG Vector\n",
        "# Turn on to enable an experimental de-JPEG-noiseing feature.\n",
        "use_jpeg_vector = False\n",
        "if use_jpeg_vector:\n",
        "  import base64\n",
        "  jpeg_weight = 0.5 #Weight of de-JPEGing effect\n",
        "\n",
        "#Chain & Parallel Optimization\n",
        "# Chains together multiple optimizers, feeding the output of each into the next.\n",
        "# Enabling this overrides the settings selected in the \"params\" section\n",
        "# The optimizer chain is a list of lists.\n",
        "# If chain_optim is true, it iterates through everything in the list-list one-by-one\n",
        "# if parallel_optim is true, then it runs every optimizer in a single list at once,\n",
        "# THEN moves onto the next element in the list-list.\n",
        "# parallel_optim does nothing without chain_optim also being enabled.\n",
        "chain_optim = True\n",
        "if chain_optim:\n",
        "    optim_chain = [\n",
        "        [{\n",
        "            \"optimizer_type\": \"AdamW\",\n",
        "            \"learning_rate\": 0.02,\n",
        "            \"min_learning_rate\": 0.01,\n",
        "            \"learning_method\": \"BOIL\",\n",
        "            \"decay\": 0.1,\n",
        "            \"iterations\": 100,\n",
        "            \"model_type\": \"VQGAN_16384\",\n",
        "            \"use_lookahead\": False,\n",
        "         },{\n",
        "            \"optimizer_type\": \"RAdam\",\n",
        "            \"learning_rate\": 0.01,\n",
        "            \"min_learning_rate\": 0.005,\n",
        "            \"learning_method\": \"BOIL\",\n",
        "            \"decay\": 0.005,\n",
        "            \"iterations\": 100,\n",
        "            \"model_type\": \"VQGAN_16384\",\n",
        "            \"use_lookahead\": False,\n",
        "         }],\n",
        "         [{\n",
        "            \"optimizer_type\" : \"Yogi\",\n",
        "            \"learning_rate\" : 15,\n",
        "            \"min_learning_rate\" : 15,\n",
        "            \"learning_method\": \"DEFAULT\",\n",
        "            \"decay\" : 0,\n",
        "            \"epsilon\" : 1e-6,\n",
        "            \"iterations\" : 200,\n",
        "            \"model_type\" : \"VQGAN_16384\", \n",
        "            \"use_lookahead\" : True\n",
        "          }],\n",
        "          [{\n",
        "            \"optimizer_type\" : \"RAdam\",\n",
        "            \"learning_rate\" : 8,\n",
        "            \"min_learning_rate\" : 1,\n",
        "            \"learning_method\": \"BOIL\",\n",
        "            \"decay\" : 0.005,\n",
        "            \"epsilon\" : 1e-6,\n",
        "            \"iterations\" : 400,\n",
        "            \"model_type\" : \"VQGAN_16384\",\n",
        "            \"use_lookahead\" : True\n",
        "          }],[{\n",
        "            \"optimizer_type\" : \"Yogi\",\n",
        "            \"learning_rate\" : 40,\n",
        "            \"min_learning_rate\" : 20,\n",
        "            \"learning_method\": \"BOIL\",\n",
        "            \"decay\" : 0,\n",
        "            \"epsilon\" : 1e-8,\n",
        "            \"iterations\" : 100,\n",
        "            \"model_type\" : \"VQGAN_16384\", \n",
        "            \"use_lookahead\" : False\n",
        "          }]\n",
        "    ]\n",
        "\n",
        "#Encoder/Decoder Optimizer\n",
        "# Optimizes the model rather than the image.\n",
        "# Recommended settings are to run w/ DiffGrad and the default settings\n",
        "# listed below. Run for as many iterations as you want, and then\n",
        "# on checkout, it will transfer the model to the specified file.\n",
        "vq_optim = False\n",
        "vq_lr = 0.001\n",
        "vq_dec = 0.0001\n",
        "\n",
        "file_to_transfer = 'image.jpg' # .jpg, .gif, or .mp4s accepted\n",
        "file_to_transfer = default_path + file_to_transfer\n",
        "\n",
        "# Disables normal learning methods\n",
        "if vq_optim:\n",
        "  chain_optim = False\n",
        "  for optim_cfg in optim_chain:\n",
        "    optim_cfg['learning_rate'] = 0\n",
        "    optim_cfg['min_learning_rate'] = 0\n",
        "    optim_cfg['learning_method'] = \"DEFAULT\"\n",
        "    optim_cfg['decay'] = 0\n",
        "    optim_cfg['epsilon'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htgsqZvgDpYx"
      },
      "source": [
        "# GPU information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUOAL4UcytFb",
        "outputId": "4fa8ed6b-8b8a-4f9a-a9a5-1bded3634e43"
      },
      "source": [
        "# were you lucky today?\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-8f56e98b-85a9-607d-1a34-bbcfaf582ccb)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcixx9Z3XYH"
      },
      "source": [
        "# Top (imports)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piJOg9MY7khd"
      },
      "source": [
        "!pip install kornia\n",
        "!pip install --no-deps ftfy regex tqdm\n",
        "!pip install einops\n",
        "!pip install torch_optimizer==0.1.0\n",
        "!pip install noise\n",
        "# !pip install git+https://github.com/aleju/imgaug.git\n",
        "\n",
        "import random\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch_optimizer as optim\n",
        "import numpy as np\n",
        "import noise\n",
        "# import imgaug\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import kornia\n",
        "import PIL\n",
        "from PIL import Image, ImageSequence, ImageOps, ImageEnhance, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from google.colab import output\n",
        "\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm3_VmxpAiB1"
      },
      "source": [
        "!git clone https://github.com/openai/CLIP.git\n",
        "from CLIP import clip\n",
        "\n",
        "perceptor_size = 224 \n",
        "if clip_model == 'RN50x4' or clip_model == 'RN50x16':\n",
        "  perceptor_size = 288\n",
        "\n",
        "perceptor, preprocess = clip.load(clip_model, jit=False)\n",
        "_ = perceptor.eval().requires_grad_(False)\n",
        "\n",
        "if clip_model == 'ViT-B/32' and scaler != 1:\n",
        "  num = 1 + int(scaler*7)**2\n",
        "  perceptor.visual.positional_embedding = torch.nn.Parameter(torch.nn.functional.upsample(perceptor.visual.positional_embedding.T.unsqueeze(0), (num), mode='linear', align_corners=True).squeeze(0).T)\n",
        "  perceptor_size = int(perceptor_size * scaler)\n",
        "elif clip_model == 'ViT-B/16' and scaler != 1:\n",
        "  num = 1 + int(scaler*14)**2\n",
        "  perceptor.visual.positional_embedding = torch.nn.Parameter(torch.nn.functional.upsample(perceptor.visual.positional_embedding.T.unsqueeze(0), (num), mode='linear', align_corners=True).squeeze(0).T)\n",
        "  perceptor_size = int(perceptor_size * scaler)\n",
        "else:\n",
        "  scaler = 1\n",
        "\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkg-G4kjHQT1",
        "outputId": "c4fd1300-b3d3-4396-eeae-c052bacd0163"
      },
      "source": [
        "!ls $default_path/models/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikiart_16384.ckpt  wikiart_16384.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHB7BcR1zNLZ"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlDVVMvzMUd"
      },
      "source": [
        "!pip uninstall torchtext --yes\n",
        "%cd $default_path\n",
        "!git clone https://github.com/CompVis/taming-transformers  \n",
        "%cd $default_path\n",
        "%mkdir $default_path/models/\n",
        "%cd $default_path/models/\n",
        "\n",
        "#TODO: make it not curl things already downloaded\n",
        "if vqgan_model == 'imagenet_1024' and not os.path.isfile('vqgan_imagenet_f16_1024.yaml'):\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_1024.yaml' #ImageNet 1024\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_1024.ckpt'  #ImageNet 1024\n",
        "elif vqgan_model == 'imagenet_16384' and not os.path.isfile('vqgan_imagenet_f16_16384.yaml'):\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1'\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1'\n",
        "elif vqgan_model == 'openimages_8192' and not os.path.isfile('vqgan_openimages_f8_8192.yaml'):\n",
        "  !curl -L -o vqgan_openimages_f8_8192.yaml -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #OpenImages 8192\n",
        "  !curl -L -o vqgan_openimages_f8_8192.ckpt -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #OpenImages 8192\n",
        "elif vqgan_model == 'coco' and not os.path.isfile('coco.yaml'):\n",
        "  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n",
        "  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n",
        "elif vqgan_model == 'faceshq' and not os.path.isfile('faceshq.yaml'):\n",
        "  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n",
        "  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n",
        "elif vqgan_model == 'wikiart_1024' and not os.path.isfile('wikiart_1024.yaml'):\n",
        "  !curl -L -o wikiart_1024.yaml -C - 'http://mirror.io.community/blob/vqgan/wikiart.yaml' #WikiArt 1024\n",
        "  !curl -L -o wikiart_1024.ckpt -C - 'http://mirror.io.community/blob/vqgan/wikiart.ckpt' #WikiArt 1024\n",
        "elif vqgan_model == 'wikiart_16384' and not os.path.isfile('wikiart_16384.yaml'):\n",
        "  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt'\n",
        "  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml'\n",
        "elif vqgan_model == 'sflckr' and not os.path.isfile('sflckr.yaml'):\n",
        "  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n",
        "  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR\n",
        "elif vqgan_model == 'gumbel' and not os.path.isfile('gumbel.yaml'):\n",
        "  !curl -L -o gumbel.yaml -c - 'https://heibox.uni-heidelberg.de/f/b24d14998a8d4f19a34f/?dl=1' #GUMBEL\n",
        "  !curl -L -o gumbel.ckpt -C - 'https://heibox.uni-heidelberg.de/f/34a747d5765840b5a99d/?dl=1' #GUMBEL\n",
        "\n",
        "%cd $default_path/taming-transformers/\n",
        "%pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "%pip install transformers\n",
        "import yaml\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "from taming.models import vqgan, cond_transformer\n",
        "from taming.models.vqgan import VQModel, GumbelVQ\n",
        "from taming.models.cond_transformer import Net2NetTransformer\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if use_jpeg_vector:\n",
        "    JPEG_Vector_Data = base64.b64decode(\"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\")\n",
        "    JPEG_Vector_File = open(\"JPEG_Vector.pt\", \"wb\")\n",
        "    JPEG_Vector_File.write(JPEG_Vector_Data)\n",
        "    JPEG_Vector_File.close()\n",
        "    JPEG_Vector = torch.load(\"JPEG_Vector.pt\").cuda()\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "if vqgan_model == 'imagenet_1024':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/vqgan_imagenet_f16_1024.yaml\", default_path + \"models/vqgan_imagenet_f16_1024.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'imagenet_16384':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/vqgan_imagenet_f16_16384.yaml\", default_path + \"models/vqgan_imagenet_f16_16384.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'openimages_8192':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/vqgan_openimages_f8_8192.yaml\", default_path + \"models/vqgan_openimages_f8_8192.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'coco':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/coco.yaml\", default_path + \"models/coco.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'faceshq':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/faceshq.yaml\", default_path + \"models/faceshq.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'wikiart_1024':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/wikiart_1024.yaml\", default_path + \"models/wikiart_1024.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'wikiart_16384':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/wikiart_16384.yaml\", default_path + \"models/wikiart_16384.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'sflckr':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/sflckr.yaml\", default_path + \"models/sflckr.ckpt\").to(DEVICE)\n",
        "elif vqgan_model == 'gumbel':\n",
        "  vqgan_model = load_vqgan_model(default_path + \"models/gumbel.yaml\", default_path + \"models/gumbel.ckpt\").to(DEVICE)\n",
        "\n",
        "%cd $default_path\n",
        "\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaocGDQXz3Zx"
      },
      "source": [
        " # Latent coordinate & Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDJHlbKcWZiD"
      },
      "source": [
        "def perlin(x,y,seed=0):\n",
        "    # permutation table\n",
        "    np.random.seed(seed)\n",
        "    p = np.arange(256,dtype=int)\n",
        "    np.random.shuffle(p)\n",
        "    p = np.stack([p,p]).flatten()\n",
        "    # coordinates of the top-left\n",
        "    xi = x.astype(int)\n",
        "    yi = y.astype(int)\n",
        "    # internal coordinates\n",
        "    xf = x - xi\n",
        "    yf = y - yi\n",
        "    # fade factors\n",
        "    u = fade(xf)\n",
        "    v = fade(yf)\n",
        "    # noise components\n",
        "    n00 = gradient(p[p[xi]+yi],xf,yf)\n",
        "    n01 = gradient(p[p[xi]+yi+1],xf,yf-1)\n",
        "    n11 = gradient(p[p[xi+1]+yi+1],xf-1,yf-1)\n",
        "    n10 = gradient(p[p[xi+1]+yi],xf-1,yf)\n",
        "    # combine noises\n",
        "    x1 = lerp(n00,n10,u)\n",
        "    x2 = lerp(n01,n11,u) # FIX1: I was using n10 instead of n01\n",
        "    return lerp(x1,x2,v) # FIX2: I also had to reverse x1 and x2 here\n",
        "\n",
        "def lerp(a,b,x):\n",
        "    \"linear interpolation\"\n",
        "    return a + x * (b-a)\n",
        "\n",
        "def fade(t):\n",
        "    \"6t^5 - 15t^4 + 10t^3\"\n",
        "    return 6 * t**5 - 15 * t**4 + 10 * t**3\n",
        "\n",
        "def gradient(h,x,y):\n",
        "    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n",
        "    vectors = np.array([[0,1],[0,-1],[1,0],[-1,0]])\n",
        "    g = vectors[h%4]\n",
        "    return g[:,:,0] * x + g[:,:,1] * y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCh2D8Dt8Xd"
      },
      "source": [
        "from torchvision.transforms.transforms import RandomGrayscale\n",
        "torch.cuda.empty_cache()\n",
        "!rm -rf $default_path/images\n",
        "!mkdir -p $default_path/images/frames\n",
        "!mkdir -p $default_path/images/frames2\n",
        "!rm -rf $default_path/images/{out_folder}\n",
        "!mkdir -p $default_path/images/{out_folder}\n",
        "\n",
        "class Pars(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pars, self).__init__()\n",
        "\n",
        "        if init_type == \"image\":\n",
        "          x = (torch.nn.functional.interpolate(torch.tensor(imageio.imread(init_image_path)).unsqueeze(0).permute(0, 3, 1, 2), (sideX, sideY)) / 255).cuda()\n",
        "          x = 2. * x - 1.\n",
        "        elif init_type == \"blocky\":\n",
        "            if grayscale_random:\n",
        "                x = torch.zeros(batch_size, 1, random_size, random_size, device=DEVICE).normal_(mean=.3, std=.7).clamp(-1, 1).expand(-1, 3, -1, -1)\n",
        "            else:\n",
        "                x = torch.rand(batch_size, 3, random_size, random_size, device=DEVICE).normal_(mean=.3, std=.7).clamp(-1, 1)\n",
        "            x = T.Resize((sideX, sideY))(x)\n",
        "        elif init_type == \"perlin\":\n",
        "            x = np.zeros((batch_size, 3, sideX, sideY))\n",
        "            n_channels = 3\n",
        "            octave_strength = 1\n",
        "            octave_scale = perlin_scale\n",
        "            for i in range(perlin_octaves):\n",
        "              # pnoise = PerlinNoise(octaves=octave)\n",
        "              for batch_i in range(batch_size):\n",
        "                seed_rand = int(random.random() * 1e6)\n",
        "                for c_i in range(n_channels):\n",
        "                    if grayscale_random:\n",
        "                      seed = seed_rand\n",
        "                    else:\n",
        "                      seed = c_i + seed_rand\n",
        "                \n",
        "                    xlin = np.linspace(0,octave_scale,sideX, endpoint=False)\n",
        "                    ylin = np.linspace(0,octave_scale,sideY, endpoint=False)\n",
        "                    xlin,ylin = np.meshgrid(ylin,xlin)\n",
        "                    x[batch_i, c_i, :, :] += octave_strength * perlin(xlin,ylin,seed=seed)\n",
        "              octave_strength *= perlin_persistence\n",
        "              octave_scale *= perlin_lacunarity\n",
        "\n",
        "            x = torch.tensor(x, dtype=torch.float32, device=DEVICE)\n",
        "            x -= x.min()\n",
        "            x /= x.max()\n",
        "            x = x * 2 - 1\n",
        "            x *= perlin_strength\n",
        "        elif init_type == \"constant\":\n",
        "            x = torch.full((batch_size, 3, sideX, sideY), init_val, dtype=torch.float32, device=DEVICE)\n",
        "        else:\n",
        "            self.normu = .5 * torch.randn(batch_size, 256, sideX//16, sideY//16, device=DEVICE)\n",
        "            self.normu = torch.nn.Parameter(torch.sinh(1.9 * torch.arcsinh(self.normu)))\n",
        "        if grayscale_fuzz:\n",
        "          extra_fuzz = torch.rand(batch_size, 1, sideX, sideY, device=DEVICE).normal_(mean=0, std=0.5).clip(-1,1).expand(-1, 3, -1, -1)\n",
        "        else:\n",
        "          extra_fuzz = torch.rand(batch_size, 3, sideX, sideY, device=DEVICE).normal_(mean=0, std=0.5).clip(-1,1)\n",
        "        x += extra_fuzz * extra_fuzz_strength\n",
        "        z, _, [_, _, indices] = vqgan_model.encode(x)\n",
        "        self.normu = torch.nn.Parameter(z.cuda().clone())\n",
        "\n",
        "    def forward(self):\n",
        "      # TODO: Parameterize clipping / scaling values?\n",
        "      lnoise = lats_noise * torch.randn_like(self.normu)\n",
        "      lnoise -= torch.mean(lnoise)\n",
        "      if lats_nonlinearity == \"tanh\":\n",
        "        return torch.tanh((self.normu + lnoise) / lats_scale) * lats_scale\n",
        "      elif lats_nonlinearity == \"clip\":\n",
        "        return (self.normu + lnoise).clip(-lats_scale, lats_scale)\n",
        "      else:\n",
        "        return self.normu + lnoise\n",
        "      \n",
        "def model(x):\n",
        "  o_i2 = x\n",
        "  o_i3 = vqgan_model.post_quant_conv(o_i2)\n",
        "  i = vqgan_model.decoder(o_i3)\n",
        "  return i\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "def enc_augment(into):\n",
        "    sideY, sideX = into.shape[2:4]\n",
        "    max_size = min(sideX, sideY)\n",
        "    min_size = min(sideX, sideY, perceptor_size)\n",
        "    cutouts = []\n",
        "    for ch in range(cutN):\n",
        "        size = int(torch.rand([])**1 * (max_size - min_size) + min_size)\n",
        "        offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "        offsety = torch.randint(0, sideY - size + 1, ())\n",
        "        cutout = into[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "        cutouts.append(torch.nn.functional.interpolate(cutout, (perceptor_size, perceptor_size), mode='bilinear', align_corners=True))\n",
        "        del cutout\n",
        "    cutouts = torch.cat(cutouts, dim=0)\n",
        "    cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "    return cutouts\n",
        "\n",
        "sharpness = torch.zeros((1,1,sideX//16,sideY//16)).float().to(DEVICE).requires_grad_(True)\n",
        "\n",
        "t = 0\n",
        "if text_input != '':\n",
        "  t = clip.tokenize(text_input)\n",
        "  t = perceptor.encode_text(t.cuda()).detach().clone()\n",
        "  if rhw_style_loss:\n",
        "    t  /= t.norm()\n",
        "\n",
        "text_add = 0\n",
        "if text_to_add != '':\n",
        "  text_add = clip.tokenize(text_to_add)\n",
        "  text_add = perceptor.encode_text(text_add.cuda()).detach().clone()\n",
        "  if rhw_style_loss:\n",
        "    text_add  /= text_add.norm()\n",
        "\n",
        "t_not = clip.tokenize(text_not)\n",
        "t_not = perceptor.encode_text(t_not.cuda()).detach().clone()\n",
        "\n",
        "advanced_t = 0\n",
        "if advanced_text_enabled:\n",
        "  advanced_t = perceptor.encode_text(clip.tokenize(prompt).cuda()).mean(0).unsqueeze(0).detach().clone()\n",
        "\n",
        "m_prompt = 0\n",
        "if music_enabled:\n",
        "  m_prompt = perceptor.encode_text(clip.tokenize(music_prompt).cuda()).mean(0).unsqueeze(0).detach().clone()\n",
        "\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "img_enc_list = []\n",
        "if w2 != 0:\n",
        "  if img_enc_path != '':\n",
        "    imgList = img_enc_path.split(\", \")\n",
        "    for i in imgList:\n",
        "      img_enc = 0\n",
        "      i = default_path + i\n",
        "      print(i)\n",
        "      image = imageio.imread(i)\n",
        "      if image.shape[-3] == 4:\n",
        "        image = image[:,:3]\n",
        "      if len(image.shape) != 3: \n",
        "        image = image.unsqueeze(0).tile(3,1,1)\n",
        "      img_enc = (torch.nn.functional.interpolate(torch.tensor(image).unsqueeze(0).permute(0, 3, 1, 2), (perceptor_size, perceptor_size)) / 255).cuda()[:,:3]\n",
        "      img_enc = enc_augment(img_enc)\n",
        "      img_enc = nom(img_enc)\n",
        "      img_enc = perceptor.encode_image(img_enc.cuda()).detach().clone()\n",
        "      if rhw_style_loss:\n",
        "        img_enc  /= img_enc.norm()\n",
        "      if img_mode == 'Average':\n",
        "        if random_list_encode > 0 and random_list_encode < len(imgList):\n",
        "          img_enc /= random_list_encode\n",
        "        else:\n",
        "          img_enc /= len(imgList)\n",
        "      img_enc_list.append(img_enc)\n",
        "      img_enc = 0\n",
        "random.shuffle(img_enc_list)\n",
        "\n",
        "ne_img_enc_list = []\n",
        "if w3 != 0:\n",
        "  if ne_img_enc_path != '':\n",
        "    imgList = ne_img_enc_path.split(\", \")\n",
        "    for i in imgList:\n",
        "      ne_img_enc = 0\n",
        "      image = imageio.imread(i)\n",
        "      if image.shape[-3] == 4:\n",
        "        image = image[:,:3]\n",
        "      if len(image.shape) != 3: \n",
        "        image = image.unsqueeze(0).tile(3,1,1)\n",
        "      ne_img_enc = (torch.nn.functional.interpolate(torch.tensor(image).unsqueeze(0).permute(0, 3, 1, 2), (perceptor_size, perceptor_size)) / 255).cuda()[:,:3]\n",
        "      ne_img_enc = enc_augment(ne_img_enc)\n",
        "      ne_img_enc = nom(ne_img_enc)\n",
        "      ne_img_enc = perceptor.encode_image(ne_img_enc.cuda()).detach().clone()\n",
        "      if rhw_style_loss:\n",
        "        ne_img_enc  /= ne_img_enc.norm()\n",
        "      if ne_img_mode == 'Average':\n",
        "        if ne_random_list_encode > 0 and ne_random_list_encode < len(imgList):\n",
        "          ne_img_enc /= ne_random_list_encode\n",
        "        else:\n",
        "          ne_img_enc /= len(imgList)\n",
        "      ne_img_enc_list.append(ne_img_enc)\n",
        "      ne_img_enc = 0\n",
        "random.shuffle(ne_img_enc_list)\n",
        "\n",
        "gif_img_enc = 0\n",
        "gifStore = 0\n",
        "if gif_img_enc_path != '':\n",
        "  gif_img_enc_path = default_path + gif_img_enc_path\n",
        "  !ffmpeg -i {gif_img_enc_path} -vsync 0 $default_path/images/frames/%d.jpg\n",
        "  length = len(os.listdir((default_path + '/images/frames')))\n",
        "  lenWhile = length\n",
        "  currentFrame = ''\n",
        "  while lenWhile > 0:\n",
        "    currentFrame = default_path + '/images/frames/' + str(lenWhile) + '.jpg'\n",
        "    gif_img_enc = (torch.nn.functional.interpolate(torch.tensor(imageio.imread(currentFrame)).unsqueeze(0).permute(0, 3, 1, 2), (perceptor_size, perceptor_size)) / 255).cuda()[:,:3]\n",
        "    gif_img_enc = nom(gif_img_enc)\n",
        "    gif_img_enc = perceptor.encode_image(gif_img_enc.cuda()).detach().clone()\n",
        "    if rhw_style_loss:\n",
        "      gif_img_enc  /= gif_img_enc.norm()\n",
        "    gifStore += gif_img_enc\n",
        "    gif_img_enc = 0\n",
        "    lenWhile -= 1\n",
        "  gifStore /= length\n",
        "  gif_img_enc = gifStore\n",
        "\n",
        "gif2_img_enc = 0\n",
        "gifStore = 0\n",
        "if gif2_img_enc_path != '':\n",
        "  gif2_img_enc_path = default_path + gif2_img_enc_path\n",
        "  !ffmpeg -i {gif2_img_enc_path} -vsync 0 $default_path/images/frames2/%d.jpg\n",
        "  length = len(os.listdir((default_path + '/images/frames2')))\n",
        "  lenWhile = length\n",
        "  currentFrame = ''\n",
        "  while lenWhile > 0:\n",
        "    currentFrame = default_path + '/images/frames2/' + str(lenWhile) + '.jpg'\n",
        "    gif2_img_enc = (torch.nn.functional.interpolate(torch.tensor(imageio.imread(currentFrame)).unsqueeze(0).permute(0, 3, 1, 2), (perceptor_size, perceptor_size)) / 255).cuda()[:,:3]\n",
        "    gif2_img_enc = nom(gif2_img_enc)\n",
        "    gif2_img_enc = perceptor.encode_image(gif2_img_enc.cuda()).detach().clone()\n",
        "    if rhw_style_loss:\n",
        "      gif2_img_enc  /= gif2_img_enc.norm()\n",
        "    gifStore += gif2_img_enc\n",
        "    gif2_img_enc = 0\n",
        "    lenWhile -= 1\n",
        "  gifStore /= length\n",
        "  gif2_img_enc = gifStore\n",
        "\n",
        "cdeg = np.cos(deg * np.pi / 180)\n",
        "sdeg = np.sin(deg * np.pi / 180)\n",
        "\n",
        "size = perceptor_size #ViT size\n",
        "ccrop = 2 * size #double the size for center crop\n",
        "pad = int(np.ceil(2 * ccrop* abs(cdeg) * abs(sdeg)))\n",
        "rcrop = int(np.ceil(ccrop * (abs(cdeg) + abs(sdeg)))) # random crop\n",
        "\n",
        "if deg >= 45:\n",
        "  pad = size\n",
        "  rcrop = int(np.ceil(ccrop * np.sqrt(2)))\n",
        "else:\n",
        "  pad = int(np.ceil(ccrop * abs(cdeg * sdeg)))\n",
        "  rcrop = int(np.ceil(ccrop * (abs(cdeg) + abs(sdeg)))) # random crop\n",
        "\n",
        "pad += pad % 2\n",
        "rcrop += rcrop % 2\n",
        "\n",
        "padding = torch.nn.Sequential(torchvision.transforms.Pad(padding=pad, padding_mode='reflect')).cuda()\n",
        "\n",
        "ToTensor = T.ToTensor()\n",
        "ToImage  = T.ToPILImage()\n",
        "\n",
        "def OpenImage(x, resize=None, convert=\"RGB\"):\n",
        "    if resize:\n",
        "        return ToTensor(Image.open(x).convert(convert).resize(resize)).unsqueeze(0).cuda()\n",
        "    else:\n",
        "        return ToTensor(Image.open(x).convert(convert)).unsqueeze(0).cuda()\n",
        "\n",
        "!wget -q https://i.imgur.com/9smB3ey.png -O BlueNoise-Color-1024.png\n",
        "bluenoise = OpenImage(\"BlueNoise-Color-1024.png\").mul(2).sub(1)\n",
        "\n",
        "def get_bluenoise(like):\n",
        "    b, c, h, w = like.shape\n",
        "    noise = T.RandomCrop((h,w))(bluenoise)\n",
        "    return noise\n",
        "\n",
        "def gaussian_sigma(x):\n",
        "    return 0.3 * ((x - 1) * 0.5 - 1) + 0.8\n",
        "\n",
        "augs = T.Compose([\n",
        "   T.RandomCrop(rcrop,pad_if_needed = True, padding_mode = 'reflect'),\n",
        "   T.RandomAffine(degrees=deg),\n",
        "   T.CenterCrop(ccrop),\n",
        "   T.RandomResizedCrop((size, size),scale=(size/ccrop,1.0),ratio=(1.0/1.0, 1.0/1.0), interpolation=3),\n",
        "   T.RandomOrder([\n",
        "      T.RandomHorizontalFlip(p=horizontal),\n",
        "      T.RandomVerticalFlip(p=vertical),\n",
        "      T.RandomErasing(p=random_erasing,value='random'),\n",
        "      T.RandomApply(transforms=[T.Lambda(lambda x: x + get_bluenoise(x).mul(blue_noise_intensity))], p=blue_noise_percentage),\n",
        "      T.RandomApply(transforms=[\n",
        "                T.RandomChoice([\n",
        "                    T.GaussianBlur( 3, (gaussian_sigma( 3)*0.75,gaussian_sigma( 3))),\n",
        "                    T.GaussianBlur( 5, (gaussian_sigma( 5)*0.75,gaussian_sigma( 5))),\n",
        "                    T.GaussianBlur( 7, (gaussian_sigma( 7)*0.75,gaussian_sigma( 7)))\n",
        "                ])\n",
        "            ], p=blur_probability)\n",
        "      ])\n",
        "])\n",
        "\n",
        "jumbo_augs = T.Compose([\n",
        "   T.RandomCrop(rcrop,pad_if_needed = True, padding_mode = 'reflect'),\n",
        "   T.CenterCrop(ccrop),\n",
        "   T.RandomResizedCrop((size, size),scale=(size/ccrop,1.0),ratio=(1.0/1.0, 1.0/1.0), interpolation=3)\n",
        "])\n",
        "\n",
        "itt = 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   al = (model(lats()).cpu().clip(-1, 1) + 1) / 2\n",
        "\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_mjtrZRpqX7"
      },
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WztSrRF23Rqg"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EuUz-ICNKUr"
      },
      "source": [
        "import time\n",
        "picked_img = []\n",
        "picked_ne_img = []\n",
        "current_epoch = 0\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def quantizetopalette(silf, palette, dither=False):\n",
        "    \"\"\"Convert an RGB or L mode image to use a given P image's palette.\"\"\"\n",
        "\n",
        "    silf.load()\n",
        "\n",
        "    # use palette from reference image\n",
        "    palette.load()\n",
        "    if palette.mode != \"P\":\n",
        "        raise ValueError(\"bad mode for palette image\")\n",
        "    if silf.mode != \"RGB\" and silf.mode != \"L\":\n",
        "        raise ValueError(\n",
        "            \"only RGB or L mode images can be quantized to a palette\"\n",
        "            )\n",
        "    im = silf.im.convert(\"P\", 1 if dither else 0, palette.im)\n",
        "    # the 0 above means turn OFF dithering\n",
        "\n",
        "    # Later versions of Pillow (4.x) rename _makeself to _new\n",
        "    try:\n",
        "        return silf._new(im)\n",
        "    except AttributeError:\n",
        "        return silf._makeself(im)\n",
        "\n",
        "def simplest_cb(img, percent=1):\n",
        "    out_channels = []\n",
        "    cumstops = (\n",
        "        img.shape[0] * img.shape[1] * percent / 200.0,\n",
        "        img.shape[0] * img.shape[1] * (1 - percent / 200.0)\n",
        "    )\n",
        "    for channel in cv2.split(img):\n",
        "        cumhist = np.cumsum(cv2.calcHist([channel], [0], None, [256], (0,256)))\n",
        "        low_cut, high_cut = np.searchsorted(cumhist, cumstops)\n",
        "        lut = np.concatenate((\n",
        "            np.zeros(low_cut),\n",
        "            np.around(np.linspace(0, 255, high_cut - low_cut + 1)),\n",
        "            255 * np.ones(255 - high_cut)\n",
        "        ))\n",
        "        out_channels.append(cv2.LUT(channel, lut.astype('uint8')))\n",
        "    return cv2.merge(out_channels)\n",
        "\n",
        "def imageshuff():\n",
        "  random.shuffle(img_enc_list)\n",
        "  random.shuffle(ne_img_enc_list)\n",
        "  picked_img.clear()\n",
        "  picked_ne_img.clear()\n",
        "  for i in range(random_list_encode):\n",
        "    picked_img.append(img_enc_list[i])\n",
        "  for i in range(ne_random_list_encode):\n",
        "    picked_ne_img.append(ne_img_enc_list[i])\n",
        "\n",
        "class SoftHistogram(torch.nn.Module):\n",
        "    def __init__(self, bins, min, max, sigma):\n",
        "        super(SoftHistogram, self).__init__()\n",
        "        self.bins = bins\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        self.sigma = sigma\n",
        "        self.delta = float(max - min) / float(bins)\n",
        "        self.centers = float(min) + self.delta * (torch.arange(bins).float() + 0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.unsqueeze(x, 0) - torch.unsqueeze(self.centers.to(DEVICE), 1)\n",
        "        x = torch.sigmoid(self.sigma * (x + self.delta/2)) - torch.sigmoid(self.sigma * (x - self.delta/2))\n",
        "        x = x.sum(dim=1)\n",
        "        return x\n",
        "\n",
        "SoftHist = SoftHistogram(bins=10, min=0, max=1, sigma=3)\n",
        "\n",
        "def augment(into, cutn=cutN):\n",
        "  into = padding(into)\n",
        "  p_s = []\n",
        "  if vq_optim:\n",
        "    p_s = [jumbo_augs(into) for _ in range(cutn)]\n",
        "  else:\n",
        "    p_s = [augs(into) for _ in range(cutn)]\n",
        "  into = torch.cat(p_s,0)\n",
        "  into += random_noise * torch.rand((into.shape[0], 1, 1, 1)).cuda() * torch.randn_like(into)\n",
        "  return into\n",
        "\n",
        "def postprocessing(img_filename):\n",
        "  img = Image.open(img_filename)\n",
        "  for i in postprocessing_list:\n",
        "    if i == \"palettize\":\n",
        "      palcount = int(768/len(palette))\n",
        "      palimage = Image.new('P', (16, 16))\n",
        "      palimage.putpalette(palette * palcount)\n",
        "      img = quantizetopalette(img, palimage, dither=palettize_dithering)\n",
        "      img = img.convert('RGB')\n",
        "    elif i == \"quantize\":\n",
        "      img = img.quantize(quantize_number, dither=quantize_dithering)\n",
        "      img = img.convert('RGB')\n",
        "    elif i == \"pixelate\":\n",
        "      pix_im_shape = [int(im_shape[0]/pixelate_factor), int(im_shape[1]/pixelate_factor)] #Divide the original resolution by the pixelate factor\n",
        "      img = img.resize((pix_im_shape[1], pix_im_shape[0]), resample=Image.BILINEAR) #Swap them for some god damn reason\n",
        "      img = img.resize((im_shape[1], im_shape[0]), Image.NEAREST) #Bring them back up to full size\n",
        "    elif i == \"brightness\":\n",
        "      enhancer = ImageEnhance.Brightness(img)\n",
        "      img = enhancer.enhance(brightness_factor)\n",
        "    elif i == \"contrast\":\n",
        "      enhancer = ImageEnhance.Contrast(img)\n",
        "      img = enhancer.enhance(contrast_factor)\n",
        "    elif i == \"saturation\":\n",
        "      enhancer = ImageEnhance.Color(img)\n",
        "      img = enhancer.enhance(saturation_factor)\n",
        "    elif i == \"sharpness\":\n",
        "      enhancer = ImageEnhance.Sharpness(img)\n",
        "      img = enhancer.enhance(sharpness_factor)\n",
        "    elif i == \"box_blur\":\n",
        "      img = img.filter(ImageFilter.BoxBlur(box_blur_radius))\n",
        "    elif i == \"grayscale\" or i == \"greyscale\": #for the brits\n",
        "      img = ImageOps.grayscale(img)\n",
        "    elif i == \"invert\":\n",
        "      img = ImageOps.invert(img)\n",
        "    elif i == \"flip\":\n",
        "      img = ImageOps.flip(img)\n",
        "    elif i == \"mirror\":\n",
        "      img = ImageOps.mirror(img)\n",
        "  img.save(img_filename, quality=95, subsampling=0)\n",
        "\n",
        "#TODO: Clean the shit out of this, my god\n",
        "def displ(img, num=0):\n",
        "    pil_img = T.ToPILImage()(img.squeeze())\n",
        "\n",
        "    # Save individual image with timestamp\n",
        "    current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
        "    img_filename = f'{default_path}images/{out_folder}/aleph_output{str(num)}_{current_time}.png'\n",
        "    pil_img.save(img_filename, quality=95, subsampling=0)\n",
        "\n",
        "    # Update root level image\n",
        "    img_filename = f'{default_path}images/aleph_output{str(num)}.png'\n",
        "    pil_img.save(img_filename, quality=95, subsampling=0)\n",
        "\n",
        "    ##TODO: Color correction\n",
        "    #out = simplest_cb(cv2.imread(img_filename), 10)\n",
        "    #cv2_imshow(out)\n",
        "\n",
        "    postprocessing(img_filename)\n",
        "\n",
        "    if itt % display_rate == 0:\n",
        "        if single_display:\n",
        "            display.clear_output(wait=True)\n",
        "        if display_init_on_epoch == True:\n",
        "            display.display(display.Image(img_filename))\n",
        "        else:\n",
        "          if itt != 0:\n",
        "            display.display(display.Image(img_filename))\n",
        "\n",
        "def checkin(loss):\n",
        "    if itt % save_rate == 0:\n",
        "        with torch.no_grad():\n",
        "            alnot = (model(lats()).detach().clip(-1, 1) + 1) / 2 #scaling?\n",
        "\n",
        "            batch_num = 0\n",
        "            for allls in alnot.detach():\n",
        "                displ(allls, batch_num)\n",
        "                batch_num += 1\n",
        "\n",
        "def ascend_txt():\n",
        "  out = model(lats())\n",
        "  if sharpen_pre_augment:\n",
        "    if itt % sharpen_every == 0:\n",
        "      sharp_mask   = torchvision.transforms.functional.resize(sharpness,(sideX,sideY))\n",
        "      highpass     = out - torchvision.transforms.functional.gaussian_blur(out, 3)\n",
        "      out     = HardTanh(sharp_mask * highpass + out)\n",
        "  into = augment((out.clip(-1, 1) + 1) / 2)\n",
        "  if sharpen_post_augment:\n",
        "    if itt % sharpen_every == 0:\n",
        "      sharp_mask   = torchvision.transforms.functional.resize(sharpness,(perceptor_size,perceptor_size))\n",
        "      highpass     = into - torchvision.transforms.functional.gaussian_blur(into, 3)\n",
        "      into     = HardTanh(sharp_mask * highpass + into)\n",
        "  into = nom(into)\n",
        "  iii = perceptor.encode_image(into)\n",
        "\n",
        "  prompt_timer_text = 0\n",
        "  prompt_timer_text_to_add = 0\n",
        "  prompt_timer_img = 0\n",
        "  prompt_timer_ne_img = 0\n",
        "  prompt_timer_gif = 0\n",
        "  prompt_timer_gif2 = 0\n",
        "  prompt_timer_advanced_text = 0\n",
        "\n",
        "  q = 0\n",
        "  q_list = []\n",
        "  if rhw_style_loss == False:\n",
        "    if itt >= prompt_timer_text:\n",
        "      q = q + w0*t\n",
        "    if itt >= prompt_timer_text_to_add:\n",
        "      q = q + w1*text_add\n",
        "    if itt >= prompt_timer_img:\n",
        "      if picked_img:\n",
        "        for i in picked_img:\n",
        "          q = q + w2 * i\n",
        "      else:\n",
        "        for i in img_enc_list:\n",
        "          q = q + w2 * i\n",
        "    if itt >= prompt_timer_ne_img:\n",
        "      if picked_ne_img:\n",
        "        for i in picked_ne_img:\n",
        "          q = q + w3 * i\n",
        "      else:\n",
        "        for i in ne_img_enc_list:\n",
        "          q = q + w3 * i\n",
        "    if itt >= prompt_timer_gif:\n",
        "      q = q + w4*gif_img_enc\n",
        "    if itt >= prompt_timer_gif2:\n",
        "      q = q + w5*gif2_img_enc\n",
        "    if advanced_text_enabled and itt >= prompt_timer_advanced_text:\n",
        "      q = q + advanced_t\n",
        "    if music_enabled:\n",
        "      q = q + m_prompt\n",
        "    if use_jpeg_vector:\n",
        "      q = q + JPEG_Vector*jpeg_weight\n",
        "    q = q / q.norm(dim=-1, keepdim=True)\n",
        "  else:\n",
        "    if itt >= prompt_timer_text:\n",
        "      q_list.append(w0*t)\n",
        "    if itt >= prompt_timer_text_to_add:\n",
        "      q_list.append(w1*text_add)\n",
        "    if itt >= prompt_timer_img:\n",
        "      if picked_img:\n",
        "        for i in picked_img:\n",
        "          q_list.append(w2 * i)\n",
        "      else:\n",
        "        for i in img_enc_list:\n",
        "          q_list.append(w2 * i)\n",
        "    if itt >= prompt_timer_ne_img:\n",
        "      if picked_ne_img:\n",
        "        for i in picked_ne_img:\n",
        "          q_list.append(w3 * i)\n",
        "      else:\n",
        "        for i in ne_img_enc_list:\n",
        "          q_list.append(w3 * i)\n",
        "    if itt >= prompt_timer_gif:\n",
        "      q_list.append(w4*gif_img_enc)\n",
        "    if itt >= prompt_timer_gif2:\n",
        "      q_list.append(w5*gif2_img_enc)\n",
        "    if advanced_text_enabled and itt >= prompt_timer_advanced_text:\n",
        "      q_list.append(advanced_t)\n",
        "    if music_enabled:\n",
        "      q_list.append(m_prompt)\n",
        "    if use_jpeg_vector:\n",
        "      q_list.append(JPEG_Vector*jpeg_weight)\n",
        "\n",
        "  main_weight = 10\n",
        "  subtract_weight = 5\n",
        "  if rhw_style_loss == False:\n",
        "    q_sim = torch.cosine_similarity(q, iii, -1).mean()\n",
        "    \n",
        "    loss = main_weight * q_sim\n",
        "    if subtract_weight:\n",
        "        loss -= subtract_weight * torch.cosine_similarity(t_not, iii, -1).mean()\n",
        "    if use_softhistogram:\n",
        "        loss += (SoftHist(out.reshape(-1)) / (out.shape[-1] * out.shape[-2])).std()\n",
        "        \n",
        "    return 1 - loss\n",
        "  else:\n",
        "    loss = 0\n",
        "    for i in q_list:\n",
        "      if type(i) != int and type(i) != float:\n",
        "        loss += main_weight * torch.cosine_similarity(i, iii, -1).mean()\n",
        "    if subtract_weight:\n",
        "        loss -= subtract_weight * torch.cosine_similarity(t_not, iii, -1).mean()\n",
        "    if use_softhistogram:\n",
        "        loss += (SoftHist(out.reshape(-1)) / (out.shape[-1] * out.shape[-2])).std()\n",
        "        \n",
        "    return 1 - loss\n",
        "\n",
        "  #all_s = torch.cosine_similarity(q, iii, -1)\n",
        "  #return [0, -10*all_s + 5 * torch.cosine_similarity(t_not, iii, -1)]\n",
        "\n",
        "def HardTanh(x):\n",
        "    x = x*2.0-1.0\n",
        "    x = torch.tanh(x+x**5*.5)\n",
        "    x = x*0.5+0.5\n",
        "    return x\n",
        "\n",
        "def train(optim_cfg):\n",
        "  loss = ascend_txt()\n",
        "  #loss = loss1[0] + loss1[1]\n",
        "  #loss = loss.mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  #uncomment when playing with LR\n",
        "  opt_0 = optimizer.param_groups[0]\n",
        "\n",
        "  if optim_cfg[\"learning_method\"] == \"BOIL\":\n",
        "    if opt_0['lr'] > 2:\n",
        "      opt_0['lr'] *= 0.994\n",
        "    elif opt_0['lr'] <= 2:\n",
        "      opt_0['lr'] *= 0.994 #0.8\n",
        "  if optim_cfg[\"learning_method\"] == \"SEAR\":\n",
        "    if itt > 25:\n",
        "      if opt_0['lr'] >= 1:\n",
        "        opt_0['lr'] = 0.1\n",
        "    else:\n",
        "      if itt > 100:\n",
        "        if opt_0['lr'] >= 0.1:\n",
        "          opt_0['lr'] = 0.01\n",
        "  if optim_cfg[\"learning_method\"] == \"RANDOM\":\n",
        "    opt_0['lr'] = ((random.random() + random.random()) / 2) + 0.1\n",
        "    if itt < 20:\n",
        "      opt_0['lr'] += (30 - itt)\n",
        "    elif itt > 200:\n",
        "      opt_0['lr'] *= (600/(itt + 400))\n",
        "  opt_0['lr'] = max(opt_0['lr'], optim_cfg[\"min_learning_rate\"])\n",
        "\n",
        "  if torch.abs(lats()).max() > 5:\n",
        "    for g in optimizer.param_groups:\n",
        "      g['weight_decay'] = optim_cfg[\"decay\"]\n",
        "  else:\n",
        "    for g in optimizer.param_groups:\n",
        "      g['weight_decay'] = 0\n",
        "\n",
        "  checkin(loss)\n",
        "\n",
        "def loop(optim_cfg):\n",
        "    global itt\n",
        "    for i in tqdm(range(optim_cfg[\"iterations\"])):\n",
        "      try:\n",
        "        train(optim_cfg)\n",
        "        itt += 1\n",
        "      except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "def get_optimizer(optim_cfg, mapper):\n",
        "  optim_params = [{'params': mapper, 'lr': optim_cfg[\"learning_rate\"]},{'params': sharpness, 'lr': 0.01}]\n",
        "  if vq_optim:\n",
        "    #optim_params.append({'params': vqgan_model.decoder.parameters(True), 'lr': vq_lr, 'weight_decay': vq_dec})\n",
        "    #optim_params.append({'params': vqgan_model.decoder.conv_in.parameters(True), 'lr': vq_lr, 'weight_decay': vq_dec})\n",
        "    #optim_params.append({'params': vqgan_model.encoder.parameters(True), 'lr': vq_lr, 'weight_decay': vq_dec})\n",
        "    optim_params.append({'params': vqgan_model.post_quant_conv.parameters(True), 'lr': vq_lr, 'weight_decay': vq_dec})\n",
        "    mul = 1\n",
        "    for pa in vqgan_model.decoder.parameters():\n",
        "        optim_params.append({'params': pa, 'lr': vq_lr * mul, 'decay': vq_dec})\n",
        "        mul = mul / 1.1\n",
        "  if optim_cfg[\"optimizer_type\"] == \"AdamW\":\n",
        "    optimizer_tmp = torch.optim.AdamW(optim_params, weight_decay=optim_cfg[\"decay\"])\n",
        "  elif optim_cfg[\"optimizer_type\"] == \"Ranger21\":\n",
        "    optimizer_tmp = Ranger21(optim_params, learning_rate, weight_decay = optim_cfg[\"decay\"], **ranger21_adv_opts)\n",
        "  else:\n",
        "    optimizer_tmp = getattr(optim, optim_cfg[\"optimizer_type\"], None)(optim_params, weight_decay=optim_cfg[\"decay\"])\n",
        "  if optim_cfg[\"use_lookahead\"]:\n",
        "    optimizer_tmp = optim.Lookahead(optimizer_tmp)\n",
        "  optimizer_tmp.zero_grad()\n",
        "  if \"epsilon\" in optim_cfg and optim_cfg[\"epsilon\"] != 0:\n",
        "    optimizer_tmp.param_groups[0]['eps'] = optim_cfg[\"epsilon\"]\n",
        "  return optimizer_tmp\n",
        "\n",
        "def train(optim_list):\n",
        "  loss = ascend_txt()\n",
        "  #loss = loss1[0] + loss1[1]\n",
        "  #loss = loss.mean()\n",
        "  for i in optimizers:\n",
        "    i.zero_grad()\n",
        "  loss.backward()\n",
        "  for i in range(len(optimizers)):\n",
        "    optimizers[i].step()\n",
        "    opt_0 = optimizers[i].param_groups[0]\n",
        "    if optim_cfg[\"learning_method\"] == \"BOIL\":\n",
        "      opt_0['lr'] *= boil_amt\n",
        "    if optim_cfg[\"learning_method\"] == \"WARMUP\":\n",
        "      opt_0['lr'] *= warmup_amt\n",
        "    if optim_cfg[\"learning_method\"] == \"SEAR\":\n",
        "      if itt > 25:\n",
        "        if opt_0['lr'] >= 1:\n",
        "          opt_0['lr'] = 0.1\n",
        "      else:\n",
        "        if itt > 100:\n",
        "          if opt_0['lr'] >= 0.1:\n",
        "            opt_0['lr'] = 0.01\n",
        "    if optim_cfg[\"learning_method\"] == \"RANDOM\":\n",
        "      opt_0['lr'] = ((random.random() + random.random()) / 2) + 0.1\n",
        "      if itt < 20:\n",
        "        opt_0['lr'] += (30 - itt)\n",
        "      elif itt > 200:\n",
        "        opt_0['lr'] *= (600/(itt + 400))\n",
        "    \n",
        "    if \"WARMUP\" in optim_cfg[\"learning_method\"]:\n",
        "      opt_0['lr'] = min(opt_0['lr'], optim_cfg[\"min_learning_rate\"])\n",
        "    else:\n",
        "      opt_0['lr'] = max(opt_0['lr'], optim_cfg[\"min_learning_rate\"])\n",
        "\n",
        "    if torch.abs(lats()).max() > 5:\n",
        "      for g in optimizers[i].param_groups:\n",
        "        g['weight_decay'] = optim_cfg[\"decay\"]\n",
        "    else:\n",
        "      for g in optimizers[i].param_groups:\n",
        "        g['weight_decay'] = 0\n",
        "\n",
        "  checkin(loss)\n",
        "\n",
        "def loop(optim_list):\n",
        "    global itt\n",
        "    for i in tqdm(range(maxits)):\n",
        "        train(optim_list)\n",
        "        itt += 1\n",
        "\n",
        "def get_optimizer(optim_cfg, mapper):\n",
        "  optim_params = [{'params': mapper, 'lr': optim_cfg[\"learning_rate\"]},{'params': sharpness, 'lr': 0.01}]\n",
        "  if vq_optim:\n",
        "    optim_params.append({'params': vqgan_model.decoder.parameters(True), 'lr': vq_lr/50, 'weight_decay': vq_dec})\n",
        "  if optim_cfg[\"optimizer_type\"] == \"AdamW\":\n",
        "    optimizer_tmp = torch.optim.AdamW(optim_params, weight_decay=optim_cfg[\"decay\"])\n",
        "  elif optim_cfg[\"optimizer_type\"] == \"Ranger21\":\n",
        "    optimizer_tmp = Ranger21(optim_params, learning_rate, weight_decay = optim_cfg[\"decay\"], **ranger21_adv_opts)\n",
        "  else:\n",
        "    optimizer_tmp = getattr(optim, optim_cfg[\"optimizer_type\"], None)(optim_params, weight_decay=optim_cfg[\"decay\"])\n",
        "  if optim_cfg[\"use_lookahead\"]:\n",
        "    optimizer_tmp = optim.Lookahead(optimizer_tmp)\n",
        "  optimizer_tmp.zero_grad()\n",
        "  if \"epsilon\" in optim_cfg:\n",
        "    optimizer_tmp.param_groups[0]['eps'] = optim_cfg[\"epsilon\"]\n",
        "  return optimizer_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWcpZpqkSfJL"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4eojeh0SdS5"
      },
      "source": [
        "for i in range(learning_epochs):\n",
        "  imageshuff()\n",
        "  \n",
        "  # Reset the image vector\n",
        "  lats = Pars().cuda()\n",
        "  mapper = [lats.normu]\n",
        "  \n",
        "  # Usually there will be only one optimizer chain step\n",
        "  for optim_list in optim_chain:\n",
        "    combo = \"\"\n",
        "    maxits = 0\n",
        "    for optim_cfg in optim_list:\n",
        "      combo += optim_cfg[\"optimizer_type\"] + \" \"\n",
        "      if optim_cfg[\"iterations\"] > maxits:\n",
        "        maxits = optim_cfg[\"iterations\"]\n",
        "    print(combo)\n",
        "    optimizers = [get_optimizer(optim_cfg, mapper) for optim_cfg in optim_list]\n",
        "    time.sleep(0.25)\n",
        "    itt = 0\n",
        "    loop(optim_list)\n",
        "  \n",
        "  itt = display_rate\n",
        "  with torch.no_grad():\n",
        "    alnot = (model(lats()).detach().clip(-1, 1) + 1) / 2\n",
        "\n",
        "    batch_num = 0\n",
        "    for allls in alnot.detach():\n",
        "        displ(allls, batch_num)\n",
        "        batch_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS6KSyfBJUF7",
        "cellView": "form"
      },
      "source": [
        "#@markdown \n",
        "\n",
        "if vq_optim:\n",
        "  import pathlib\n",
        "  init_type = \"image\"\n",
        "  extension = pathlib.Path(file_to_transfer).suffix\n",
        "\n",
        "  if extension == '.gif' or extension == '.mp4':\n",
        "    !rm -rf $default_path/images/l25v_output/*\n",
        "    !rm -rf $default_path/images/frames3\n",
        "    !mkdir $default_path/images/frames3\n",
        "\n",
        "    !ffmpeg -i {gif2_img_enc_path} -vsync 0 $default_path/images/frames3/%d.jpg\n",
        "    length = len(os.listdir(default_path + 'images/frames3'))\n",
        "\n",
        "    for i in range(length):\n",
        "      img_input.append(f(default_path + 'images/frames3/{i+1}.jpg'))\n",
        "\n",
        "    for i in range(length):\n",
        "      print(i)\n",
        "      init_image_path = img_input[i]\n",
        "      lats = Pars().cuda()\n",
        "  elif extension == '.jpg':\n",
        "    init_image_path = file_to_transfer\n",
        "    lats = Pars().cuda()\n",
        "    checkin(0)\n",
        "  init_type = \"perlin\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRFirL0Ea1p0"
      },
      "source": [
        "checkin(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vNYuZmlX9uP"
      },
      "source": [
        "# Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvO9RS5rX9Z_"
      },
      "source": [
        "if auto_video:\n",
        "    !rm $default_path/video.mp4\n",
        "    # Choose frame rate and final save path (in Colab's filesystem)\n",
        "    fps = 30\n",
        "    save_path = default_path + 'video.mp4'\n",
        "    image_path = default_path + 'images/l25v_output/*.png'\n",
        "\n",
        "    # Create video\n",
        "    !ffmpeg -r {fps} -pattern_type glob -i '{image_path}' -vcodec libx264 -crf 15 -pix_fmt yuv420p {save_path} -hide_banner -loglevel error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K7q526YYEB6"
      },
      "source": [
        "if auto_video:\n",
        "    # Display video\n",
        "    mp4 = open(save_path,'rb').read()\n",
        "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "    display.HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9day8bCmYEhe"
      },
      "source": [
        "#!zip -q -r $default_path/file.zip $default_path/images/l25v_output/ && echo 'success' || echo 'failure'\n",
        "#from google.colab import files\n",
        "#download_path = default_path + 'file.zip'\n",
        "#files.download(download_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwhPlSdZE5UV"
      },
      "source": [
        "# Thanks/Credits!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKP0tnHaiTyl"
      },
      "source": [
        "**Latent Vision Credits (written by @advadnoun on twitter)**\n",
        "\n",
        "Many thanks to OpenAI for releasing their models CLIP and DALL-E (the encoder and decoder parts, specifically). I am not affiliated with them.\n",
        "\n",
        "https://github.com/openai/DALL-E/ (Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever)\n",
        "\n",
        "https://github.com/openai/CLIP (Alec Radford, \\* Jong Wook Kim,\\* Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
        "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever)\n",
        "\n",
        "Also, as a good launching point for future directions and to find more related work, see https://distill.pub/2017/feature-visualization/ by Chris Olah, Alexander Mordvintsev, Ludwig Schubert.\n",
        "\n",
        "Thanks to to Patrick Esser\\* Robin Rombach\\* Bjorn Ommer for the VQGAN in Taming Transformers! https://github.com/CompVis/taming-transformers\n",
        "\n",
        "\\* equal contribution\n",
        "\n",
        "**Also massive thanks to my patrons and contributors on the Discord who support me**. You'll also see that alstroemeria313 (https://twitter.com/RiversHaveWings), CobaltOwl, and Johanezz have contributed the suggestions of weight decay and random-encoded images specifically.\n",
        "\n",
        "Thanks to ZoeOzone for the title of this notebook: https://twitter.com/zoebot_zoe\n",
        "\n",
        "Thanks to hotgrits for loading CLIP without restarting. http://torridgristle.tumblr.com/\n",
        "\n",
        "Props to spruce (https://twitter.com/kingchloexx), who did previous work on adding encoded text to an encoded image using Aleph.\n",
        "\n",
        "Haltakov's post & notebook introduced me to CLIP arythmetic: https://twitter.com/advadnoun/status/1386742901300817920\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "**Zoetrope Credits (written by @ai_curio on twitter)**\n",
        "\n",
        "Zoetrope made by Bearsharktopusdev. (https://twitter.com/ai_curio)\n",
        "\n",
        "Improved augmentation code/pipeline by Kram. (https://twitter.com/eduwatch2)\n",
        "\n",
        "Many of the advanced parameters, as well as the image saving/video generation code, and delightful mentorship, by Danielrussruss (https://twitter.com/danielrussruss)\n",
        "\n",
        "Sharpen filter and general code assistance by hotgrits (http://torridgristle.tumblr.com/)\n",
        "\n",
        "Many fixes and general code cleaning from Nyrt (https://twitter.com/RoborosewaterM)\n",
        "\n",
        "Many fixes and general code cleaning from Dekxi (https://twitter.com/deKxi)\n",
        "\n",
        "Bugfixes from LesbianChemicalPlant (https://lesbianchemicalplant.tumblr.com/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVxF_28CEjRR"
      },
      "source": [
        "# Old Patch Notes\n",
        "## What's new between Zoetrope and Zoe2trope?\n",
        "\n",
        "I've integrated [Kram](https://twitter.com/eduwatch2)'s new augmentation code and parameterization and added several interesting presets, along with the ability to delay adding prompts to the generator until a certain iteration is hit. I've also added a large amount of parameters, including some ported stuff from danielrussruss's 2.5v, to make this User Friendly and Easy.\n",
        "\n",
        "## What's new between Zoe2trope and Zo3trope?\n",
        "\n",
        "A whole lot of code optimizations and advanced parameters, mostly courtesy of [hotgrits](http://torridgristle.tumblr.com/), Kram, and [danielrussruss](https://twitter.com/danielrussruss), with assorted contributions from members of A Latent Space. Training epochs! The ability to feed it .pngs and .mp4s! The ability to ask it to randomly select from a list of images! Bugfixes! All that good stuff.\n",
        "\n",
        "## What's new between Zo3trope and Zoetrope 4?\n",
        "Further code optimizations, more advanced parameters, the option to use the Gumbel model and the ability to easily change optimizer type with code courtesy of [Nyrt](https://twitter.com/RoborosewaterM). Added some clarifying stuff to the options, and got a code cleanup, as well as the fun new feature to chain optimizers together.\n",
        "\n",
        "## What's new between Zoetrope 4 and Zoetrope 5?\n",
        "Nyrt continues to be an excellent assistant by using the fact that they are way better at python than I am to clean up my code. Reworked some of the init options to be more sensible and added Perlin Noise, which is a more regular, blobby sort of noise. Added blue noise augmentations in advanced parameters and renamed some of the options - up_noise is now random_noise and random_noise is now random_erasing. Fixed some display issues w/ the bar. Danielrussruss aided in adding vq optimization in Advanced Parameters for very fast medium-quality style transferral to large batches of images and videos. Eudaimon helped added parallel optimization."
      ]
    }
  ]
}