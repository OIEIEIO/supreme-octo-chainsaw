{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMTL-Scrape.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxFrY971FPgpfrwClndqFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OIEIEIO/supreme-octo-chainsaw/blob/main/HMTL_Scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZQxW1SRiWzo"
      },
      "outputs": [],
      "source": [
        "# The code below will download the HTML code for the website and store it in a string called `html_doc`. It will then use the `BeautifulSoup` library to parse the HTML code, and create a BeautifulSoup object that stores all the content in a nested structure.\n",
        "# \n",
        "# Run the cell below to complete the download and parsing. Then, run the next cell to see the HTML code.\n",
        "import requests\n",
        "html_doc = requests.get('https://www.popsci.com/').text\n",
        "from bs4 import BeautifulSoup\n",
        "bs = BeautifulSoup(html_doc, 'html.parser')\n",
        "print(bs.prettify())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's find the images.\n",
        "# \n",
        "# In the cell below:\n",
        "# \n",
        "# * Create a list called `images` and set it equal to all the images inside the `bs` object.\n",
        "# * Use the `prettify` method to print out the first 100 characters of the html document.\n",
        "images = bs.find_all('img')\n",
        "print(bs.prettify()[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Dkp2cqjX9v",
        "outputId": "822e24f7-7114-48e5-b2a2-6b4816edc347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en-US\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <meta content=\"ie=edge\" http\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Great! Now we've extracted the images.  Next, let's create a folder called `washington_post_images` and download all the images from the `images` list into this folder. \n",
        "# \n",
        "# In the cell below, complete the code so that the images are downloaded.\n",
        "import os\n",
        "\n",
        "# Create a folder called washington_post_images\n",
        "os.makedirs('popsci_post_images', exist_ok=True)"
      ],
      "metadata": {
        "id": "hQfwxNkgqqJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through images and download them \n",
        "for i in images:\n",
        "    image_url = i['src']\n",
        "    # Download each image and save it to the folder\n",
        "    print(image_url)"
      ],
      "metadata": {
        "id": "rL8aXDFBjs_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the requests library, you make a request to a web server and in return you get the HTML document.  Let's see how this works.\n",
        "# \n",
        "# In the cell below:\n",
        "# \n",
        "# * Assign the URL of interest to the variable `url` \n",
        "# * Use the `requests.get()` method to make an HTTP request to `url`\n",
        "# * Use the `text` attribute of the object returned by `requests.get()` to return the HTML of the webpage as a string\n",
        "# * Assign this string to `html_doc`\n",
        "# * Display `html_doc`\n",
        "url = 'https://www.popsci.com/'\n",
        "html_doc = requests.get(url).text\n",
        "print(html_doc)"
      ],
      "metadata": {
        "id": "qBKUjm3hlD-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, what is returned is the HTML of the webpage as a string.  With this string, we can use BeautifulSoup to create a BeautifulSoup object, which lets us traverese through HTML and extract other strings.\n",
        "# \n",
        "# Run the cell below to create a BeautifulSoup object called `bs` using `html_doc`.\n",
        "bs = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "# Now, let's try extracting the title of the webpage.\n",
        "# \n",
        "# In the cell below, use the `find` method to find the first `<h1>` tag on the page.  Use the `text` method to extract the text within the `<h1>` tag.  Do the following in order:\n",
        "# \n",
        "# * Assign the name `title` to the result of the `find` method\n",
        "# * Display `title`\n",
        "title = bs.find('h1')\n",
        "print(title)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w39Im_volWEW",
        "outputId": "cb677954-63a4-4597-9dd1-8c2c20120f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Great!  Now we've extracted the title.  \n",
        "# \n",
        "# In the cell below, let's extract the text of the first paragraph of the webpage.  Notice that the text we want to extract is within the first `<p>` tag.\n",
        "# \n",
        "# * Use the `find` method to find the first `<p>` tag\n",
        "# * Use the `text` method to extract the text within the `<p>` tag\n",
        "# * Assign the result to `first_paragraph`\n",
        "# * Display `first_paragraph`\n",
        "first_paragraph = bs.find('p')\n",
        "print(first_paragraph.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkfefNeLljI1",
        "outputId": "22ca1414-a986-476d-b1d0-c4c2e6ba2b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're in control, not Instagram.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's try extracting all the paragraphs on the page.  In the cell below:\n",
        "# \n",
        "# * Use the `find_all` method to find all the `<p>` tags\n",
        "# * Create a for loop to loop through the tags and extract the text in the paragraphs. \n",
        "# * Append the result to `paragraphs`\n",
        "# * Display `paragraphs`\n",
        "paragraphs = bs.find_all('p')\n",
        "for p in paragraphs:\n",
        "    print(p.text)"
      ],
      "metadata": {
        "id": "ITxgjHM_nmCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Great! Now we've extracted the text from the first paragraph.  Now, let's try extracting the text from the entire webpage.  In the cell below:\n",
        "# \n",
        "# * Create a for loop to loop through the `<p>` tags\n",
        "# * Append the result to `paragraphs`\n",
        "# * Display `paragraphs`\n",
        "paragraphs = []\n",
        "for p in bs.find_all('p'):\n",
        "    paragraphs.append(p.text)\n",
        "print(paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh-uXziWnv2U",
        "outputId": "75177182-785a-4ec0-d067-f8e2ebd4b92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"You're in control, not Instagram.\", 'Officials are increasingly treating woodsmoke as a public health and environmental justice issue, despite opposition.', 'Tips and tricks for the Apple, Google, and Microsoft contact apps.', 'Behind the wheel of Mach-E GT, a fast EV with a stiff feel.', \"The first station will be off the coast of Curaçao, and it's scheduled to be completed by 2025.\", 'People have been spreading misinformation since the dawn of media. ', 'What to know about military hardware in Ukraine: Strela anti-air missiles, NLAW anti-tank weapons, and the TB-2 drone.', 'This is what’s next for Birdwatch, Twitter’s crowdsourced tool for tackling misinformation. ', \"HEIC files are efficient, but they aren't always compatible.\", 'Reduce, reuse, then maybe rent. ', 'Keep your digital property safe.', 'The US and many other countries have decided to ban Russian aircraft from their airspace, a move that could \"cripple\" Russian carriers. ', 'Here’s why—and what to do if you have one.', 'Like science, tech, and DIY projects?', \"Sign up to receive Popular Science's emails and get the highlights.\", 'We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.\\nRegistration on or use of this site constitutes acceptance of our Terms of Service.', '© 2022 Recurrent. All rights reserved.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's try extracting the text of the first five headers on the webpage.  In the cell below:\n",
        "# \n",
        "# * Create a for loop to loop through the first five `<h2>` tags\n",
        "# * Append the result to `headers`\n",
        "# * Display `headers`\n",
        "headers = []\n",
        "for h in bs.find_all('h2')[:5]:\n",
        "    headers.append(h.text)\n",
        "print(headers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YGDLtjZn4bp",
        "outputId": "6ae15ab5-36b0-4a2d-ea1a-583b9d8c167c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n\\nScience\\n', '\\n\\nTech\\n', '\\n\\nSpace\\n', '\\n\\nLife Hacks\\n', '\\n\\nAnimals\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Now, let's try extracting the text of the first five headers on the webpage.  In the cell below:\n",
        "# \n",
        "# * Create a for loop to loop through the first five `<h2>` tags\n",
        "# * Append the result to `headers`\n",
        "# * Display `headers`\n",
        "headers = []\n",
        "for h in bs.find_all('h3')[:5]:\n",
        "    headers.append(h.text)\n",
        "print(headers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qytEw3bIoSXv",
        "outputId": "4af55fa7-94b5-4079-eceb-06861322dd0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Perfect your aesthetic by choosing your own Instagram Story background', 'What is a ‘Martian flower’?', 'The Amazon is on the brink of a climate change tipping point', 'The Rafale fighter jet is a warplane with a ‘French touch’', 'An EPA fix for pollution-spewing wood stoves is backfiring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's create a dictionary that contains the text from all the `<h2>` tags on the page, and the number of times they appear.  In the cell below:\n",
        "# \n",
        "# * Create an empty dictionary called `h2_dict`\n",
        "# * Loop through all the `<h2>` tags\n",
        "# * Extract the text from each tag\n",
        "# * If the tag has a parent, or comes immediately after another tag, the text will be a duplicate-- get rid of this by making the text blank\n",
        "# * Increment the count for the tag in the dictionary by one\n",
        "# * Create a for loop to loop through the keys of the dictionary and print each one, along with the number of times it appears in the document\n",
        "# * Experiment with adding a key to the dictionary, then removing it using the `.pop` method\n",
        "# * Try to add a key that doesn't exist in the document\n",
        "# * Try to remove a key that doesn't exist in the document\n",
        "h2_dict = {}\n",
        "for h in bs.find_all('h2'):\n",
        "    text = h.text\n",
        "    if h.find_next_sibling():\n",
        "        text = ''\n",
        "    h2_dict[text] = h2_dict.get(text, 0) + 1\n",
        "for key, val in h2_dict.items():\n",
        "    print(f'{key}: {val}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Qdy5xFoiVY",
        "outputId": "e7349c41-ae32-4743-8449-e9911ca75ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": 13\n",
            "What we know about the firefight at Europe’s biggest nuclear power plant on Friday: 1\n",
            "Earthquake models get a big shakeup with clues buried in the San Andreas fault: 1\n",
            "Why you hate going back to work after vacation: 1\n",
            "What happens if you eat too many Tums?: 1\n",
            "Ford added more power to its electric Mustang Mach-E. Here’s how it drives.: 1\n",
            "Jacques Cousteau’s grandson is building a network of ocean floor research stations : 1\n",
            "Deepfakes may use new technology, but they’re based on an old idea: 1\n",
            "These are the weapons in the Ukrainian arsenal: 1\n",
            "Twitter’s fact-checking program might be headed to your feed: 1\n",
            "How to convert HEIC files—or prevent them altogether: 1\n",
            "Does renting your clothes reduce fashion waste?: 1\n",
            "Secure your Microsoft account so it’s hard to get into: 1\n",
            "How Biden’s airspace ban will affect Russian airlines: 1\n",
            "Fitbit is recalling its Ionic smartwatch after wearers report burns: 1\n"
          ]
        }
      ]
    }
  ]
}